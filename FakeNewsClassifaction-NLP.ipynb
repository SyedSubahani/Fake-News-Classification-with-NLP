{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275de936",
   "metadata": {},
   "source": [
    "\n",
    "# Problem Statement :\n",
    "## Fake News Classification with The Help Of Natural Language Processing Technique.\n",
    "Fake news detection is a hot topic in the field of natural language processing. We consume news through several mediums throughout the day in our daily routine, but sometimes it becomes difficult to decide which one is fake and which one is authentic. Our job is to create a model which predicts whether a given news is real or fake.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "id: unique id for a news article\n",
    "\n",
    "title: the title of a news article\n",
    "\n",
    "author: author of the news article\n",
    "\n",
    "text: the text of the article; could be incomplete\n",
    "\n",
    "label: a label that marks the article as potentially unreliable\n",
    "\n",
    "  1: unreliable\n",
    "  \n",
    "  0: reliable\n",
    " \n",
    " ### Data Collection\n",
    "- Dataset Source - https://zenodo.org/record/4561253/files/WELFake_Dataset.csv?download=1\n",
    "- The data consists of 5 column and 20800 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4ad7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer,CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from tqdm import tqdm # printing the status bar\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.pandas.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17baaa38",
   "metadata": {},
   "source": [
    "### 1. Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1cebbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (20800, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/News_dataset.csv\")\n",
    "\n",
    "print(\"Shape: \", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5049e038",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Data Checks to perform\n",
    "\n",
    "- Check Missing values\n",
    "- Check Duplicates\n",
    "- Check data type\n",
    "- Check the number of unique values of each column\n",
    "- Check statistics of data set\n",
    "- Check various categories present in the different categorical column\n",
    "\n",
    "### 2.1 Check Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98907eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20800 entries, 0 to 20799\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      20800 non-null  int64 \n",
      " 1   title   20242 non-null  object\n",
      " 2   author  18843 non-null  object\n",
      " 3   text    20761 non-null  object\n",
      " 4   label   20800 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 812.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab36247d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.isna().sum()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f3f5f3",
   "metadata": {},
   "source": [
    "<b>Observation:</b>  There are missing values in the given dataset\n",
    "- 558 titles features are missing\n",
    "- 1957 author details are missing\n",
    "- 39 text features are missing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8382131e",
   "metadata": {},
   "source": [
    "### 2.2 Check duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1828794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0962aebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'author', 'text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf168064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate values Features wise:\n",
      "text:  413\n"
     ]
    }
   ],
   "source": [
    "print(\"Duplicate values Features wise:\")\n",
    "#print(\"Title: \", df[\"title\"].duplicated().sum())\n",
    "#print(\"Author: \", df[\"author\"].duplicated().sum())\n",
    "print(\"text: \", df[\"text\"].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04c29b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>169</td>\n",
       "      <td>Mohamad Khweis: Another “Virginia Man” (Palest...</td>\n",
       "      <td>James Fulford</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>295</td>\n",
       "      <td>A Connecticut Reader Reports Record Voter Regi...</td>\n",
       "      <td>VDARE.com Reader</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>470</td>\n",
       "      <td>BULLETIN: There ARE Righteous Jews For Trump!;...</td>\n",
       "      <td>admin</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>480</td>\n",
       "      <td>Watch: Muslim ‘Palestinians’ Declare “We follo...</td>\n",
       "      <td>admin</td>\n",
       "      <td>jewsnews © 2015 | JEWSNEWS | It's not news...u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>573</td>\n",
       "      <td>Le top des recherches Google passe en top des ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20728</th>\n",
       "      <td>20728</td>\n",
       "      <td>Trump warns of World War III if Clinton is ele...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Email Donald Trump warned in an interview Tues...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20749</th>\n",
       "      <td>20749</td>\n",
       "      <td>Realities Faced by Black Canadians are a Natio...</td>\n",
       "      <td>Anonymous</td>\n",
       "      <td>Tweet Widget by Robyn Maynard \\nCanada, includ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20750</th>\n",
       "      <td>20750</td>\n",
       "      <td>Why Did Four Googles Kill This White?</td>\n",
       "      <td>Andrew Anglin</td>\n",
       "      <td>Migrant Crisis Disclaimer \\nWe here at the Dai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20754</th>\n",
       "      <td>20754</td>\n",
       "      <td>No More American Thanksgivings</td>\n",
       "      <td>Glen Ford</td>\n",
       "      <td>Thanksgiving by Glen Ford \\n“The core ideologi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20769</th>\n",
       "      <td>20769</td>\n",
       "      <td>WORLD WAR 3 ▲ Mr.President #004 ▲ xFrozenLPx</td>\n",
       "      <td>Pakalert</td>\n",
       "      <td>source Add To The Conversation Using Facebook ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "169      169  Mohamad Khweis: Another “Virginia Man” (Palest...   \n",
       "295      295  A Connecticut Reader Reports Record Voter Regi...   \n",
       "470      470  BULLETIN: There ARE Righteous Jews For Trump!;...   \n",
       "480      480  Watch: Muslim ‘Palestinians’ Declare “We follo...   \n",
       "573      573  Le top des recherches Google passe en top des ...   \n",
       "...      ...                                                ...   \n",
       "20728  20728  Trump warns of World War III if Clinton is ele...   \n",
       "20749  20749  Realities Faced by Black Canadians are a Natio...   \n",
       "20750  20750              Why Did Four Googles Kill This White?   \n",
       "20754  20754                     No More American Thanksgivings   \n",
       "20769  20769       WORLD WAR 3 ▲ Mr.President #004 ▲ xFrozenLPx   \n",
       "\n",
       "                 author                                               text  \\\n",
       "169       James Fulford                                                      \n",
       "295    VDARE.com Reader                                                      \n",
       "470               admin                                                      \n",
       "480               admin  jewsnews © 2015 | JEWSNEWS | It's not news...u...   \n",
       "573                 NaN                                                NaN   \n",
       "...                 ...                                                ...   \n",
       "20728               NaN  Email Donald Trump warned in an interview Tues...   \n",
       "20749         Anonymous  Tweet Widget by Robyn Maynard \\nCanada, includ...   \n",
       "20750     Andrew Anglin  Migrant Crisis Disclaimer \\nWe here at the Dai...   \n",
       "20754         Glen Ford  Thanksgiving by Glen Ford \\n“The core ideologi...   \n",
       "20769          Pakalert  source Add To The Conversation Using Facebook ...   \n",
       "\n",
       "       label  \n",
       "169        1  \n",
       "295        1  \n",
       "470        1  \n",
       "480        1  \n",
       "573        1  \n",
       "...      ...  \n",
       "20728      1  \n",
       "20749      1  \n",
       "20750      1  \n",
       "20754      1  \n",
       "20769      1  \n",
       "\n",
       "[413 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"text\"].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f0560d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title 2.68  % missing values\n",
      "author 9.41  % missing values\n",
      "text 0.19  % missing values\n"
     ]
    }
   ],
   "source": [
    "## check the percentage of null values present in each feature\n",
    "feature_na =[feature for feature in df.columns if df[feature].isnull().sum() > 0]\n",
    "\n",
    "for feature in feature_na:\n",
    "    print(feature, np.round(df[feature].isnull().mean(),4)*100, \" % missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2d264e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_na = df[df[\"text\"].isnull()]\n",
    "# Deleteting the rows where text features is null\n",
    "df.drop(df[df[\"text\"].isnull()].index, inplace = True, axis=0)\n",
    "df.drop(df[df[\"id\"].isnull()].index, inplace = True, axis=0)\n",
    "#Update the other features null values with \"missing\"\n",
    "df[\"title\"].fillna(\"missing\", inplace=True)\n",
    "df[\"author\"].fillna(\"missing\", inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23685f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20761 entries, 0 to 20760\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   index   20761 non-null  int64 \n",
      " 1   id      20761 non-null  int64 \n",
      " 2   title   20761 non-null  object\n",
      " 3   author  20761 non-null  object\n",
      " 4   text    20761 non-null  object\n",
      " 5   label   20761 non-null  int64 \n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 973.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f9a4f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting data according to text in ascending order\n",
    "sorted_data=df.sort_values('text', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "872592f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19596</th>\n",
       "      <td>19635</td>\n",
       "      <td>19635</td>\n",
       "      <td>14 Days to Do 14 Things, If You Want to Surviv...</td>\n",
       "      <td>Dave Hodges</td>\n",
       "      <td>\\n\\n \\nUPDATE: HILLARY CLINTON IS AGAIN UNDER ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16433</th>\n",
       "      <td>16467</td>\n",
       "      <td>16467</td>\n",
       "      <td>Russian Report Warns: American Revolution Has ...</td>\n",
       "      <td>The European Union Times</td>\n",
       "      <td>\\nA sobering new Security Council ( SC ) analy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19811</th>\n",
       "      <td>19850</td>\n",
       "      <td>19850</td>\n",
       "      <td>Sick Hillary Needed a Doctor in the Oval Offic...</td>\n",
       "      <td>Brenda Walker</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6207</th>\n",
       "      <td>6220</td>\n",
       "      <td>6220</td>\n",
       "      <td>Radio Derb: Peak White Guilt, PC Now To The LE...</td>\n",
       "      <td>John Derbyshire</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20203</th>\n",
       "      <td>20242</td>\n",
       "      <td>20242</td>\n",
       "      <td>Radio Derb Transcript For October 21 Up: The M...</td>\n",
       "      <td>John Derbyshire</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>519</td>\n",
       "      <td>519</td>\n",
       "      <td>US NATO To Attack Putin Military Drills in Rus...</td>\n",
       "      <td>Pakalert</td>\n",
       "      <td>source Add To The Conversation Using Facebook ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10322</th>\n",
       "      <td>10343</td>\n",
       "      <td>10343</td>\n",
       "      <td>World war 3 Update &amp; Death of Petrodollar</td>\n",
       "      <td>Pakalert</td>\n",
       "      <td>source Add To The Conversation Using Facebook ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6877</th>\n",
       "      <td>6891</td>\n",
       "      <td>6891</td>\n",
       "      <td>Germany Forming EU Super Army Preparing For Wo...</td>\n",
       "      <td>Pakalert</td>\n",
       "      <td>source Add To The Conversation Using Facebook ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5041</th>\n",
       "      <td>5052</td>\n",
       "      <td>5052</td>\n",
       "      <td>zentak – World War 3 is coming Russia,USA,Chin...</td>\n",
       "      <td>Pakalert</td>\n",
       "      <td>source Add To The Conversation Using Facebook ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16832</th>\n",
       "      <td>16866</td>\n",
       "      <td>16866</td>\n",
       "      <td>missing</td>\n",
       "      <td>Anonymous</td>\n",
       "      <td>yOU HAVE TO DRAW THE LINE SOMEWHERE AND CHILD ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index     id                                              title  \\\n",
       "19596  19635  19635  14 Days to Do 14 Things, If You Want to Surviv...   \n",
       "16433  16467  16467  Russian Report Warns: American Revolution Has ...   \n",
       "19811  19850  19850  Sick Hillary Needed a Doctor in the Oval Offic...   \n",
       "6207    6220   6220  Radio Derb: Peak White Guilt, PC Now To The LE...   \n",
       "20203  20242  20242  Radio Derb Transcript For October 21 Up: The M...   \n",
       "...      ...    ...                                                ...   \n",
       "518      519    519  US NATO To Attack Putin Military Drills in Rus...   \n",
       "10322  10343  10343          World war 3 Update & Death of Petrodollar   \n",
       "6877    6891   6891  Germany Forming EU Super Army Preparing For Wo...   \n",
       "5041    5052   5052  zentak – World War 3 is coming Russia,USA,Chin...   \n",
       "16832  16866  16866                                            missing   \n",
       "\n",
       "                         author  \\\n",
       "19596               Dave Hodges   \n",
       "16433  The European Union Times   \n",
       "19811             Brenda Walker   \n",
       "6207            John Derbyshire   \n",
       "20203           John Derbyshire   \n",
       "...                         ...   \n",
       "518                    Pakalert   \n",
       "10322                  Pakalert   \n",
       "6877                   Pakalert   \n",
       "5041                   Pakalert   \n",
       "16832                 Anonymous   \n",
       "\n",
       "                                                    text  label  \n",
       "19596  \\n\\n \\nUPDATE: HILLARY CLINTON IS AGAIN UNDER ...      1  \n",
       "16433  \\nA sobering new Security Council ( SC ) analy...      1  \n",
       "19811                                                         1  \n",
       "6207                                                          1  \n",
       "20203                                                         1  \n",
       "...                                                  ...    ...  \n",
       "518    source Add To The Conversation Using Facebook ...      1  \n",
       "10322  source Add To The Conversation Using Facebook ...      1  \n",
       "6877   source Add To The Conversation Using Facebook ...      1  \n",
       "5041   source Add To The Conversation Using Facebook ...      1  \n",
       "16832  yOU HAVE TO DRAW THE LINE SOMEWHERE AND CHILD ...      1  \n",
       "\n",
       "[375 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_data[sorted_data[\"text\"].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c1cdbc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10387\n",
       "1    10374\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many positive and negative reviews are present in our dataset?\n",
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e51778",
   "metadata": {},
   "source": [
    "# 3.  Text Preprocessing.\n",
    "\n",
    "Now that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n",
    "\n",
    "Hence in the Preprocessing phase we do the following in the order below:-\n",
    "\n",
    "1. Begin by removing the html tags\n",
    "2. Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "3. Check if the word is made up of english letters and is not alpha-numeric\n",
    "4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n",
    "5. Convert the word to lowercase\n",
    "6. Remove Stopwords\n",
    "7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>\n",
    "\n",
    "After which we collect the words used to describe positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "998f75fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd2ab10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/sebleier/554280\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "# <br /><br /> ==> after the above steps, we are getting \"br br\"\n",
    "# we are including them into stop words list\n",
    "# instead of <br /> if we have <br/> these tags would have revmoved in the 1st step\n",
    "\n",
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "595e4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus):\n",
    "    preprocessed = []\n",
    "    for sentance in tqdm(corpus):\n",
    "        #sentance = re.sub(r\"http+\", \"\", sentance)\n",
    "        sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "        sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "        sentance = decontracted(sentance)\n",
    "        sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "        sentance = re.sub('[^A-Za-z]+', ' ', sentance)    \n",
    "        sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "        preprocessed.append(sentance.strip())\n",
    "\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "966e9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "lm = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range (len(df)):\n",
    "    review = re.sub('^a-zA-Z0-9',' ', df['title'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lm.lemmatize(x) for x in review if x not in stopwords]\n",
    "    review = \" \".join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7aa7ef31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'house dem aide: didn’t even see comey’s letter jason chaffetz tweeted'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c71eb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20761/20761 [00:22<00:00, 918.15it/s] \n",
      "100%|██████████| 20761/20761 [00:05<00:00, 4061.35it/s]\n",
      "100%|██████████| 20761/20761 [00:04<00:00, 5149.37it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preprocessed_text = preprocess(df['text'].values)\n",
    "preprocessed_title = preprocess(df['title'].values)\n",
    "preprocessed_author = preprocess(df['author'].values)\n",
    "\n",
    "#print(preprocessed_text[1500])\n",
    "#print(preprocessed_title[1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a2ae434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>house dem aide even see comey letter jason cha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>ever get feeling life circles roundabout rathe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>truth might get fired october tension intellig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>videos civilians killed single us airstrike id...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>print iranian woman sentenced six years prison...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  id                                              title  \\\n",
       "0      0   0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1      1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2      2   2                  Why the Truth Might Get You Fired   \n",
       "3      3   3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4      4   4  Iranian woman jailed for fictional unpublished...   \n",
       "\n",
       "               author                                               text  \\\n",
       "0       Darrell Lucus  house dem aide even see comey letter jason cha...   \n",
       "1     Daniel J. Flynn  ever get feeling life circles roundabout rathe...   \n",
       "2  Consortiumnews.com  truth might get fired october tension intellig...   \n",
       "3     Jessica Purkiss  videos civilians killed single us airstrike id...   \n",
       "4      Howard Portnoy  print iranian woman sentenced six years prison...   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      0  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"] = preprocessed_text\n",
    "#df[\"title\"] = preprocessed_title\n",
    "#df[\"author\"] = preprocessed_author\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e592495",
   "metadata": {},
   "source": [
    "## 4.  Featurization\n",
    "\n",
    "### BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1576f4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some feature names  ['aa', 'aaa', 'aaaaah', 'aaaaggg', 'aaaahhh', 'aaah', 'aaahhh', 'aaajiao', 'aaany', 'aaas']\n",
      "==================================================\n",
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (20761, 145197)\n",
      "the number of unique words  145197\n"
     ]
    }
   ],
   "source": [
    "##BoW\n",
    "count_vect = CountVectorizer() #in scikit-learn\n",
    "count_vect.fit(preprocessed_text)\n",
    "print(\"some feature names \", count_vect.get_feature_names()[:10])\n",
    "print('='*50)\n",
    "\n",
    "final_counts = count_vect.transform(preprocessed_text)\n",
    "print(\"the type of count vectorizer \",type(final_counts))\n",
    "print(\"the shape of out text BOW vectorizer \",final_counts.get_shape())\n",
    "print(\"the number of unique words \", final_counts.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785bd9d5",
   "metadata": {},
   "source": [
    "### Bi-Grams and n-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46e4d28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (20761, 5000)\n",
      "the number of unique words including both unigrams and bigrams  5000\n"
     ]
    }
   ],
   "source": [
    "#bi-gram, tri-gram and n-gram\n",
    "\n",
    "#removing stop words like \"not\" should be avoided before building n-grams\n",
    "# count_vect = CountVectorizer(ngram_range=(1,2))\n",
    "# please do read the CountVectorizer documentation http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# you can choose these numebrs min_df=10, max_features=5000, of your choice\n",
    "count_vect = CountVectorizer(ngram_range=(1,2), min_df=10, max_features=5000)\n",
    "final_bigram_counts = count_vect.fit_transform(preprocessed_text)\n",
    "print(\"the type of count vectorizer \",type(final_bigram_counts))\n",
    "print(\"the shape of out text BOW vectorizer \",final_bigram_counts.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", final_bigram_counts.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859af66",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2169083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some sample features(unique words in the corpus) ['aa', 'aa superluminal', 'aaa', 'aaron', 'aaron klein', 'aaron rodgers', 'aaronkleinshow', 'aaronkleinshow follow', 'aarp', 'ab']\n",
      "==================================================\n",
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer  (20761, 105469)\n",
      "the number of unique words including both unigrams and bigrams  105469\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "tf_idf_vect.fit(preprocessed_text)\n",
    "print(\"some sample features(unique words in the corpus)\",tf_idf_vect.get_feature_names()[0:10])\n",
    "print('='*50)\n",
    "\n",
    "final_tf_idf = tf_idf_vect.transform(preprocessed_text)\n",
    "print(\"the type of count vectorizer \",type(final_tf_idf))\n",
    "print(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98d4aa9",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caeded77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your own Word2Vec model using your own text corpus\n",
    "i=0\n",
    "list_of_sentance=[]\n",
    "for sentance in preprocessed_text:\n",
    "    list_of_sentance.append(sentance.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebcc5010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('incredible', 0.6327919363975525), ('good', 0.6126022934913635), ('amazing', 0.5777631402015686), ('proud', 0.5656858086585999), ('wonderful', 0.5285385251045227), ('fantastic', 0.5174841284751892), ('perfect', 0.4875513017177582), ('love', 0.4818818271160126), ('greatest', 0.4774805009365082), ('greatness', 0.4755479693412781)]\n",
      "==================================================\n",
      "[('biggest', 0.5728533267974854), ('greatest', 0.5631301999092102), ('horrible', 0.5441107153892517), ('nightmare', 0.5412285327911377), ('dangerous', 0.5383963584899902), ('darkest', 0.5321401357650757), ('terrible', 0.5290238857269287), ('hardest', 0.5124144554138184), ('strongest', 0.5079717040061951), ('best', 0.5046460628509521)]\n"
     ]
    }
   ],
   "source": [
    "# Using Google News Word2Vectors\n",
    "\n",
    "# in this project we are using a pretrained model by google\n",
    "# its 3.3G file, once you load this into your memory \n",
    "# it occupies ~9Gb, so please do this step only if you have >12G of ram\n",
    "# we will provide a pickle file wich contains a dict , \n",
    "# and it contains all our courpus words as keys and  model[word] as values\n",
    "# To use this code-snippet, download \"GoogleNews-vectors-negative300.bin\" \n",
    "# from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "# it's 1.9GB in size.\n",
    "\n",
    "\n",
    "# http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.W17SRFAzZPY\n",
    "# you can comment this whole cell\n",
    "# or change these varible according to your need\n",
    "\n",
    "is_your_ram_gt_16g=False\n",
    "want_to_use_google_w2v = False\n",
    "want_to_train_w2v = True\n",
    "\n",
    "if want_to_train_w2v:\n",
    "    # min_count = 5 considers only words that occured atleast 5 times\n",
    "    w2v_model=Word2Vec(list_of_sentance,min_count=5)\n",
    "    print(w2v_model.wv.most_similar('great'))\n",
    "    print('='*50)\n",
    "    print(w2v_model.wv.most_similar('worst'))\n",
    "    \n",
    "elif want_to_use_google_w2v and is_your_ram_gt_16g:\n",
    "    if os.path.isfile('GoogleNews-vectors-negative300.bin'):\n",
    "        w2v_model=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        print(w2v_model.wv.most_similar('great'))\n",
    "        print(w2v_model.wv.most_similar('worst'))\n",
    "    else:\n",
    "        print(\"you don't have gogole's word2vec file, keep want_to_train_w2v = True, to train your own w2v \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8acb2580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words that occured minimum 5 times  53120\n",
      "sample words  ['said', 'not', 'mr', 'trump', 'one', 'would', 'people', 'new', 'clinton', 'no', 'like', 'also', 'president', 'time', 'state', 'us', 'could', 'many', 'even', 'years', 'states', 'two', 'first', 'government', 'american', 'world', 'last', 'obama', 'united', 'news', 'hillary', 'year', 'get', 'may', 'campaign', 'country', 'election', 'ms', 'going', 'make', 'way', 'u', 'house', 'made', 'white', 'back', 'know', 'much', 'media', 'think']\n"
     ]
    }
   ],
   "source": [
    "w2v_words = list(w2v_model.wv.index_to_key)\n",
    "\n",
    "print(\"number of words that occured minimum 5 times \",len(w2v_words))\n",
    "print(\"sample words \", w2v_words[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65167269",
   "metadata": {},
   "source": [
    "### Converting text into vectors using wAvg W2V, TFIDF-W2V\n",
    "\n",
    "#### Avg W2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23a28d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20761/20761 [34:55<00:00,  9.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20761\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in tqdm(list_of_sentance): # for each review/sentence\n",
    "    sent_vec = np.zeros(100) # as word vectors are of zero length 100, you might need to change this to 300 if you use google's w2v\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4946a4",
   "metadata": {},
   "source": [
    "#### TFIDF weighted W2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "311229cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\n",
    "model = TfidfVectorizer()\n",
    "model.fit(preprocessed_text)\n",
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "dictionary = dict(zip(model.get_feature_names(), list(model.idf_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4601ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 533/20761 [16:40<10:33:01,  1.88s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-28a1c150a251>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# for each word in a review/sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw2v_words\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtfidf_feat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m#             tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;31m# to reduce the computation we are\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = model.get_feature_names() # tfidf words/col-names\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "for sent in tqdm(list_of_sentance): # for each review/sentence \n",
    "    sent_vec = np.zeros(100) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vec = w2v_model.wv[word]\n",
    "#             tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n",
    "            # to reduce the computation we are \n",
    "            # dictionary[word] = idf value of word in whole courpus\n",
    "            # sent.count(word) = tf valeus of word in this review\n",
    "            tf_idf = dictionary[word]*(sent.count(word)/len(sent))\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
